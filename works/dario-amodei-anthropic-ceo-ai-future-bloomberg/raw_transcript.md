Can we start with the kind of rough status of how the industry is going? You and your rivals, how close are we now to artificial general intelligence? Set the stage for us first and then we can talk about the morality of it all. Yeah. Yeah. Thanks for. Thanks for having me. I mean, you know, I've never liked the artificial general intelligence or superintelligence. Not because I don't think is very powerful. Right. I'm not I'm not a skeptic. I'm actually extreme in terms of my views of how powerful the technology is going to be. But it's the wrong model for thinking about it, that there will be some one point where we build something completely different. What we see here, actually, what we've seen for the last ten years, maybe even 15, and you know, me and therapist co-founders were among the first to document it is this very smooth, exponential, exponential process. Just like in the nineties, you saw Moore's Law that, you know, the the amount of computing power would double every 12 months or every 18 months. We have a moore's Law like law, except it's for intelligence itself, for for the cognitive ability of the model across many tasks. And so what we are seeing is that that cognitive ability is, you know, depending on how you measure it, doubling every, you know, four to 4 to 12 months. And so, you know, we're just climbing the ladder of cognitive ability. And where we are now is, you know, to take coding as one example that's been exploding really fast over the last year or two. I have engineers. In fact, the team that leads one of our lead products called Code, which is the way you use our models for for coding. He says he hasn't written any code in the last two months. He's it's all caught he's he's edited it he's looked at it but it's all been written by Claude. Indeed. We recently released this thing called Coachwork, which was bringing code code to, you know, non-coding tasks. It really seems to have taken off. We wrote it in like a week and a half, almost entirely with code code. So I think we're entering the world where, you know, the junior level software engineers, maybe many of the tasks of the more senior level software engineers are starting to be done, you know, most of the way by buying AI systems now that's going to go further can be more end to end the project to me more. But I think we're going to be surprised at how the exponential turns upward. Right? The whole the whole thing about exponentials is, you know, it looks like it's going very, very slowly. It speeds up a little bit and then it just zooms past you. And I think we're I think we're on the precipice. I think we're a year or two away from it, really zooming past us. So it's very like going bankrupt. But when when this happens in terms of the kind of running order does yourself, this Gemini, this open air, where do you put you all together at the moment roughly? Yeah. I mean, you know, so I think I think as the field has developed, you know, I think it's no longer appropriate to think of it as like, you know, like a scalar, you know, like runners in a race who are at one position. The players have gone in different directions. So some of the other players have gone into very consumer oriented direction. And that leads you to make your models, you know, super humanly engaging or super humanly good at recommending shopping or ads or various things like that. Anthropic has focused, I think, first and foremost on enterprises, developers. And to the extent we do consumer, we're very focused on productivity and the kind of high value end of the of the consumer work. And so that that gives very that gives very different incentives. Right? If you if you want to get things done. You know, we talked about AGI in superintelligence before. One of the things I would say is that, you know, there are superintelligence is today and they're basically large corporations, right? They're smarter than any human can be, you know, solving the problem of, you know, shipping, shipping commerce at the lowest possible cost or making solar panels the lowest possible cost or launching rockets at the lowest possible cost. And so there's really high returns to intelligence in this area, which I think incentivizes us to to kind of to kind of build the right thing. It's it's also, you know, I think it's I think it's a business that is more stable than consumer. We don't need you know, we don't need ads. We don't need, you know, large numbers of free users. We can we can just very directly create value. Right? There aren't all these weird externalities of, like your prioritizing engagement. You're prioritizing these other things, you're generating all this slop. We just make stuff that that people can use. Year ago here, you what? You said the Chinese were catching up a bit. Do you think now the Chinese have fallen behind? You know, I think they never really caught up that much. So, you know, of course, there was this huge excitement around deep. Yeah, right. But, you know, the truth was a couple of things. One. You know, those models are very optimized for the benchmarks. It's actually very easy to optimize the model for a finite list of benchmarks. When we go out into the world, right, when we're when we're when we're, you know, competing against other companies for enterprise contracts, we see just honestly and candidly, we see Google and we see Openai. Every once in a while, we see a couple other U.S. players. I have almost never lost a deal, lost a contract to to a Chinese model. But now you have the Trump administration, and I think you've already protested about this, giving high speed chips and video chips to the Chinese. That's right. That's right. The thing that is holding them back and they've said it themselves. The CEOs of these companies say it's the embargo on chips that's holding us back. They explicitly say this. And now, indeed, you know, there are some policies and I hope they change their mind to, you know, to explicitly send not quite our latest generation of chips, although it was reported that even that was being considered. But, you know, the generation of chips, that's that's just one back that's still extremely powerful. And we are many years ahead of China in terms of our in terms of our ability to make chips. So I think it would be a big mistake to ship these chips. You know, the the analogy I thought of, if you think about the incredible national security implications of building model, building models that are essentially cognition, that are essentially intelligence. Right. I've called where we're going with this, a country of geniuses in the data center, right. So imagine 100,000, 100 million people smarter than any Nobel Prize winner. And it's going to be under the control of one, one country or another. So so I think this is crazy. I think it's you know, it's a bit like, you know, I don't know, like selling, selling, you know, nuclear weapons to North Korea and, you know, bragging. Oh, yeah, But we made that. You know, your friend your friend David Sachs is basically arming the Chinese. You know, I wouldn't refer to any particular people, but, you know, I would I would just say that this particular policy, I think, is is akin to. Yeah, not not, not, not, I think well-advised. Another issue in this, you know, could be you could be right all this technology be going in the right direction but economically we could be heading towards a bubble. Do you think that's true? Yeah. Yeah. So I would separate out the two things. One is the basic exponential, and that's what we've been talking about. That's been kind of the technological direction. And there, you know, just like with Moore's Law, you can never be sure that the technology is sort of fully going to advanced. It's going to keep going. Right? It's a fundamentally inductive problem. But now that I've seen well over a decade of it, I would say I am pretty confident that it's going to continue, or at least I'm more confident than I've ever been at any time in the past that it's going to continue to where the models are basically smarter than humans at almost everything. And I think there's a good chance that even happens in the next year or two. Again, it seems far away. But but this property of exponential that they like, they they they catch you by surprise. And if it's not a year or two, I think it's pretty likely that it's, you know, at least less than five years within the 2020s. I think this moment will come in the 2020. So so, you know, kind of I'll get into that economic angle. Yeah, I'll get to the economic diffusion, which I think is complicated, but I don't want anyone to forget like that. You know that as long as we have that fundamental technological gain, like there's going to be many trillions of revenue, maybe many trillions per company in this in this area, because the economic potential is so great. Now there's a separate thing, which is that we don't know how fast enterprises companies are actually going to be able to use this technology. Right. The the the uses of this, what the technology is capable of today is probably ten times what the enterprise of the world are able to are able to deploy. And I see it with our customers every day where I'll talk to a CEO and they'll understand and their executive team will understand the power of this technology, you know, for for automating customer service, for coding, for many other things. But they have a company of tens of thousands of people who are perfectly brilliant people but expert in something that is not A.I. and have to learn to use A.I. and that you know, different names for it change management, enterprise transformation, whatever it's called the need to do. This is very slow. It can take years. And so we have this powerful technology that I have an incredible amount of confidence will generate trillions in revenue. But we don't know exactly when, you know, plus, plus or minus a few years. We don't know exactly when. And and and in the meantime, companies have to buy compute to serve all that revenue. And you don't want to buy too much because, you know, you could financially overextend yourself and you don't want to buy too little because then you can't serve all the revenue. And so that's where the kind of economic and financial problems. So you are involved in building data centers, spending vast fortunes. You've got the deals with Belvedere. So you worry that that's going to come back and bite you. So so we have some advantages in this. There is a stability to enterprise business that there is not to consumer, right. Consumers, very fickle enterprise purchasing enterprise predictions are predictable. Also, enterprise tends to have better margins. The margin is basically how much buffer you have between buying too little and buying too much between what you have to pay for and what you have to support in revenue. So we have a number of advantages here. But I you know, I won't deny that there is some inherent risk in any in all of the companies to this process. Whenever you have an upside whose amount is uncertain or even whose timing is uncertain, and you have to make financial decisions somewhat blind to those things, you have to you have to decide early to, you know, to to build out capital, to build out resources a few years in advance. There's going to be some inherent risk that's driven by that uncertainty. And I do think that some companies have probably overbought. I'm not able to look at their financials. I just know what they announce. And I'm like, Whoa, I wouldn't necessarily have done that. I'm pretty happy with the decisions that we've made. But, you know, and I can't be I can't be sure about others, but it is entirely consistent that this could be the most transformative technology in the history of the world, that some companies will do really well. But but not every company may very quickly on that. As you said, you own the enterprise side. On the consumer side, Openai with a big lead. You know, there seems to be look to Gemini as an eye catching up. Is that your impression as well? I mean, you know, we're not in that market, so I so I don't know about that. Yeah. You know, the statistics would seem to suggest that it would seem to suggest that, you know, with their kind of greater distribution, with their greater existing reach, they've been able to make a lot of progress in. Kind of recent times. And again, this is why, you know, consumers fickle. This is why I would be careful if, you know, if I were a consumer company in in in you know, in kind of you know, what you know in what was on my balance sheet. Let's jump then. It's that big question which we originally asked Claude at the beginning, will I do do do good. Obviously, as you said, it's going to have a positive effect on GDP, perhaps on your wealth, too. But what about employment? Last year you predicted the kind of white collar bloodbath. You said it would wipe out 50% of entry level jobs. That's now four and a half years away. Do you still stand by that? Yeah. So, you know, I you know, my view of of of I there's maybe two axes in terms of which two think there's good things happening versus bad things happening. And there's like A.I. is a small deal versus A.I. is a big deal. You know, my view is on the extreme side of I is a big deal, but I'm kind of in both side, both both top quadrants where I think some really good things will happen. And if we don't act to prevent them, some really bad things will happen. You know, we you know, I wouldn't be building this technology if I didn't believe that the good outweighed the bad and that we could mitigate the bad. That's why I warn about the bad, so that we could We can. How do you how do you think we mitigate it? Does that mean things like taxation? Yeah. Well, let me let me just get to what I think the kind of the situation is. You know, I think exactly what you said. I think we could have this very unusual combination of very fast GDP growth and high unemployment or at least underemployment or, you know, low wage job, a lot of low wage jobs, high inequality. I don't think that's a macro correct me if I'm wrong. I don't think that's a macroeconomic combination we've ever seen before. You think of fast growth, you're like, well, okay, maybe there's inflation, but you know, you're not going to have high unemployment when there's fast growth. I think this technology is a bit different because it's extreme in the way in the way it's going to it's going to it's going to generate value. But also because it's moving up the cognitive waterline, there's going to be, unfortunately, a whole class of people who are who are, I think, across a lot of industries going to have a hard time coping. And that's really a problem we need to solve. So to get to the question you just asked, there's a few things Anthropic is doing. One is we maintain for almost a year now what we call the anthropic economic index, which is tracking how our models are used in real time. And because we see all these conversations, we can use quite itself to in a privacy preserving manner. Look across all the conversations and ask questions like, Is someone using this to augment a task to work together with the model or to delegate or fully automated task? What are the industries people are using called for statistically by distribution? What are the subtasks within those industries? Right. We can get into, you know, a lot of these like lead, you know, these very fine grained Labor Department statistics and ask all about it. Which states are using cloud more? What is we can watch the economic diffusion of cloud in real time. And and so the reason to do all this is that I don't think you can make good policy if you don't have the right data. And I worry that the data that the government produces as comprehensive as it is, just doesn't move fast enough and isn't detailed enough for this. So so, you know, that is that is kind of our first contribution. We are thinking increasingly about this kind of I don't know if you want to call it retraining or ability to help people adapt. It's it's partly in line with kind of what we do in terms of go to market, Right. Part of our go to market activity is how to help you with that, but with great risk, great respect. I mean, you can do these things, but fundamentally it's a societal problem. You know, government, if you think about it, what you're describing is a perfect storm. You have GDP going up, the wealth of certain people going massively. Absolutely. And you are saying 50% of entry level jobs go people in real distressed. You are surely going to see a political. Change coming from this. And my personal bet would be, yes, you're going to get people trying to restrain the tech barons, but you would also get people saying, let's have higher taxes. We need to pay for this. Do you think higher taxes are coming? So, you know, my view again, you know, I was going to the kind of voluntary stuff we could do first, but but actually, I actually do believe that. I think this one is going to be big enough that, you know, at some point I think everyone's going to come to the realization that there needs to be some kind of macroeconomic intervention there. You know, if we if we look at just the disparities in wealth that we have now, if we look at it as a fraction of GDP, I believe we've kind of exceeded the Gilded Age already. And this is mostly without without I. So I think things are going to go even further. And so, yeah, you know, my my guess is this is this is not even going to be a partisan thing. There's a wealth there's a wealth tax now in California. Are you supportive of that? No, I think it's a great start. I think that's poorly designed. And, you know, my my my worry would be if we don't think about these things in a sober way, then we'll get things that are that are you know, that are that are kind of poorly designed. So, you know, that would be that would be my message to, you know, the others in the world who are doing who are doing well in this boom is that, you know, if we if we don't think proactively about how to make this revolution work for everyone, we will get these proposals that don't make sense. Just taking it a bit further, there's you know, people worry about the economic consequences, but there are other ones. Not that long ago I went to a panel with, I think a couple of your rivals, a couple of your peers, where they again, they put forward all the good things I would do. And then someone said, what about the risks? And one of them rather casually said, well, of course there's extinction risk and then moved on to another. And collectively around the room, people took a deep breath. And what he was talking about was the possibility of a bad actor getting hold of it, or the possibility ultimately of things like Claud running things. What does that fit into your. Yeah, yeah. You know, I've long been been worried about that as well. You know, just, just as I said, you know, I'm on the direction of like, this is a very powerful technology. Therefore the benefits are extreme. We're going to be able to, you know, do things like really seriously, really cure cancer or maybe eradicate tropical diseases. But on the other side, look, we're building cognitive systems that have their own autonomy, Right. And we really need to think about that. And lycanthropy has been founded since the beginning with an intention to think about that. Right. You know, we publish probably almost every, you know, probably three or four times a month research on how to control these models, how to make sure that, you know, they they do what we want them to do and they don't run out of control. We've pioneered one of my co-founders, Chris Ola, is, you know, kind of arguably, you know, the inventor of a field called mechanistic interpretability, which is basically when you look inside the brain, the artificial brain of of Claude or another A.I. model and try to trace mechanistically why it's doing exactly what it's doing. And we've seen things inside the model like, you know, in in lab environments, you know, sometimes the models will develop the intent to blackmail, the intent to deceive. And this isn't unique to court. If anything, this is worse in other models. These are things that if we don't train the models in the right way can emerge. And but we've pioneered the science of looking inside them so we can diagnose them and and, you know, prevent the models from happening, intervene and retrain the models so they don't behave in this way. That said, you know, we this is something we have to be careful about. We've we've, you know, supported various what, you know, light touch, transparency focused measures for making sure that, you know, all the companies have to talk about the tests that they run. We always disclose our tests. We always, you know, basically we try and test our models to try and really stream them and get them to do the worst things possible in a test environment so that they never do those things in the real world. And we generally think every company should have to disclose or should have to run those tests and should have to disclose those tests. So this is you spend quite a lot of time thinking about this. You write pamphlets about it, about the consequences. There is an impression that most of your peers are going to head down and they're thinking purely about whether they can just keep ahead of the other. Do you think your industry is kind of immature in that way? You know, I can't speak to what the other players are are doing or why they're doing it. You know, I think there are at least some other players in in the ecosystem who I, you know, I think of as responsible who are at least thinking of things in the right way. I agree there are others who are not. But. I would say what I have always tried to do and what Anthropic has always tried to do is, is to set an example and to try to inspire others to follow the example. We always publish our work on mechanistic interpretability. We always publish the tests we run. And honestly, there are a lot of other companies where, you know, their researchers basically say, Hey, why can't why can't we do that too? That seems like a responsible thing to do. That seems like the right way to develop the technology. And so you can kind of have an effect on the other players in the ecosystem. 111 very clear zone where you are different from your peers that you have not very obviously queued up to kiss the ring of Donald J. Trump. Are you still? What do you think of the current US President? You showed no signs of wanting to see him. Look, so, I mean, you know, I wish I would. I would really put it in a different way. I don't think being for or against administrations or for or against politicians is is the right approach or that anthropic has anything to say one way or another on those topics. What I would say is that what Anthropic knows is I Anthropic knows very well the policy issues around A.I. and our approach, which, you know, I can't speak for others, but you know, indeed, indeed our approach may have been different is we think through the issues, we try and get a substantive view based on those issues, and then we say what we think. And sometimes we will disagree with the current administration, just as we sometimes just like with China, with the last administration, and sometimes we agree. And it's worth highlighting the areas of agreement like the energy data center buildouts. We did this health pledge at the White House. You know, there are a number of areas, you know, when we saw the you know, the White House's air action plan last summer, actually, we liked most of that. That was actually a very well-written document. So this is complex. There's not against and for. But again, when it comes to things like the chips in China, yeah, we disagree when it comes to putting a moratorium on state deregulation. Yeah, we disagree because we disagree on the merits. It's not about liking or disliking a particular person. I don't I don't think that that that kind of thinking is going to get us out of this situation that we're in. We have to think based on substance. Are you going to see him when he comes to Davos tomorrow? I may. I think that would be an interesting change. One final last question. What happens to entropic? What happens? Do you have a I think what since we have supposedly a 350 billion implied valuation, would you consider an IPO this year? Look, I mean, you know, I we are we are most focused on, you know, just trying to make the best models and just trying to, you know, build products on top of those models and just trying to, you know, sell those models to enterprises in ways that are useful, you know, their their large capital needs for that for the for the field. So that that should be considered. But, you know, that's where our focus is. That implies that you do need a lot of money though. I mean, you don't need to ask me to know that this is a capital. So an IPO isn't completely out of the question and it's never completely out of the question. Those the IPO and Donald Trump with both possibles in your life. Derek Derek, thank you very much for talking to us. We now know all about mechanistic interoperability and we've had a view inside your brain and it's not a bad one. Thank you. Thank you for having me, John.
